---
title: 'Homework 3: Categorical Data'
author: "Nicholas G Reich, for Biostats 743 at UMass-Amherst"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning=FALSE, eval=FALSE)
```

Your assignment should be submitted in two separate files by 8am on Wednesday October 4th. The first, should be an RMarkdown (.Rmd) or another format that dynamically compiles your write up and runs the code inside it. The second should be the PDF file that was reproducibly compiled using the first file. All figures should be generated by the code, none should be loaded directly. The homework files should be submitted using your shared Google Drive folder with the instructor. 


# Understanding likelihood-based inference

## Question 1

Assume the following data generation model and write code to simulate data from this model:
$$ Y_i \sim Poisson(\mu_i) $$
where 
$$log \mu_i = \alpha + \beta \cdot x_i$$
Fix $\alpha=log(15)$, $\beta = log(2)$, and draw $x_i$ independently from the distribution $Unif(0, 20)$ for $i=1, ..., N$.

(a) Simulate $N=500$ observations from this model. Plot the data in a reasonable way.
(b) Fit a Poisson log-linear GLM to the data.
(c) Write a function that can calculate the likelihood of your data given $(\alpha, \beta)$. Across a fine grid of possible values for $\alpha$ and $\beta$, compute the likelihood for each point. Plot the resulting likelihood surface, showing the MLE, contour lines representing the 80% and 95% likelihood based credible regions, and the estimated 80% and 95% confidence ellipses based on the estimated asymptotic covariance of $(\hat\alpha, \hat\beta)$.
(d) Repeat a-c for a new sample of $N=20$.
(e) Describe your results, with particular attention paid to (i) comparisons between the confidence/credible regions for each of the two sample sizes, and (ii) any differences you observe across the two different samples of data. Note: be sure to use `set.seed()` to ensure your results and interpretations are reproducible and consistent when you re-knit the file. 
(e) Re-run the code for (d) 10 times (with different samples of $y$ each time, hold $x$ fixed across the iterations) and qualitatively assess the sensitivity of your results in (d) and interpretations in (e). Do your results change substantially with each new sample?

## Question 2

Gibbs sampling with censoring? ARM (402). Using a Poisson model?

ROC curve

rstanarm

Exercise 5.8 (about smoothing)?

Compare penalized likelihood to bayesian 2x2 table simulation?

## Question 3

conditional logit

## Question 4

This question uses the `frisk_with_noise.dat` dataset in the `CDA` Google Drive folder. The data resulted from an investigation by the NY State Attorney General's Office into the "stop and frisk" policies of the NYPD. These data are an anonymized version of that data, collected over a 15-month period in 1998 and 1999. For this analysis, we are interested in looking at the impact of ethnicity and precinct on the rate of police stops.

(a) Based on this data, does ethnicity play a role in rate of police stopping? **Before** running any models, look at the data available to you and write down a one-paragraph analysis plan that describes what models you will fit. For this section, do not consider any models with overdispersion. Include in your write-up, the analysis plan, summary output of key models that you fit, and a one-paragraph summary of the results. Use mathematical notation to write down the model equations for at least one of the fitted models.

(b) For one of the chosen models from part (a), investigate whether a correction for overdispersion is needed. If it is, fit a new model with the correction in place. Show the results, describe how and why the results changed (if they did) and and re-interpret the results in light of this model.

(c) Considering the analysis plan that you wrote in (a) if you had it to do again, is there anything you would change about the analysis plan if you had to do it over again? Why?
